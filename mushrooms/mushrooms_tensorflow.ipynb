{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushrooms classification - Tensorflow\n",
    "\n",
    "This notebook is used to build and train a binary classification neural network. We will train it for the [Kaggle - Mushroom Classification](https://www.kaggle.com/uciml/mushroom-classification) project. The data should be already processed and saved to .npy files. See the `mushrooms_data_preparation.ipynb` notebook from this repository to process the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import used modules\n",
    "\n",
    "The only external modules used for this notebook are [numpy](http://www.numpy.org/), [tensorflow](https://www.tensorflow.org/), and [matplotlib](https://matplotlib.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SCRIPT_DIR = Path.cwd()\n",
    "sys.path.append(str(SCRIPT_DIR.parent))\n",
    "\n",
    "from data_utils.data_iterator import DataIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the processed data\n",
    "\n",
    "First we need to load the processed data. This files are loaded as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(open(\"training_data/mushrooms_training_data.npy\", 'rb'))\n",
    "training_labels = np.load(open(\"training_data/mushrooms_training_labels.npy\", 'rb'))\n",
    "validation_data = np.load(open(\"training_data/mushrooms_validation_data.npy\", 'rb'))\n",
    "validation_labels = np.load(open(\"training_data/mushrooms_validation_labels.npy\", 'rb'))\n",
    "test_data = np.load(open(\"training_data/mushrooms_test_data.npy\", 'rb'))\n",
    "test_labels = np.load(open(\"training_data/mushrooms_test_labels.npy\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print data type and arrays shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data type = <class 'numpy.ndarray'>\n",
      "Training data shape = (6500, 117)\n",
      "Training labels shape = (6500, 1)\n",
      "Validation data shape = (812, 117)\n",
      "Validation labels shape = (812, 1)\n",
      "Test data shape = (812, 117)\n",
      "Test labels shape = (812, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded data type = {}\".format(type(training_data)))\n",
    "print(\"Training data shape = {}\".format(training_data.shape))\n",
    "print(\"Training labels shape = {}\".format(training_labels.shape))\n",
    "print(\"Validation data shape = {}\".format(validation_data.shape))\n",
    "print(\"Validation labels shape = {}\".format(validation_labels.shape))\n",
    "print(\"Test data shape = {}\".format(test_data.shape))\n",
    "print(\"Test labels shape = {}\".format(test_labels.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "Now we will define our neural network model. We will create a tensorflow graph that we can later train. This graph should have:\n",
    "- **Placeholders**: These are the model inputs. When training or evaluating the model, we will feed the graph through this operations with out data.\n",
    "- **Hidden layers**: In this model we will use a stack of [fully connected layers](https://www.tensorflow.org/api_docs/python/tf/layers/dense). The number, size and activation function of this layers can be changed from the model parameters variables.\n",
    "- **Output layer**: We will use a fully connected layer, with size 1 (one neuron) and a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation function (the standart output function for binary classification).\n",
    "- **Cost computation**: We need a tensorflow operation to calculate the cost of the current model prediction, with respect to the real labels. We will use the [logarithmic loss](https://datawookie.netlify.com/blog/2015/12/making-sense-of-logarithmic-loss/) for this task.\n",
    "- **Training operations**: This tensorflow operations will be used to perform one training step (gradients calculation and parameters update) so the model decrease the cost.\n",
    "- **Summaries**: We will add [Tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard) summaries to vizualize the model graph (architecture), the training process, and some parameters of the model.\n",
    "- **Metrics**: We will add operations to check the performance of the model with respect to the true labels we are trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters\n",
    "\n",
    "This parameters are used to easily change the model architecture to try different configurations and find the best for our specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "fully_connected_layers = 3\n",
    "fully_connected_size = 50\n",
    "fully_connected_activation = tf.nn.relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create placeholders\n",
    "\n",
    "The placeholders are created using the data shapes. The first dimension is `None`, because this is the batch size, and we want to change it for different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"inputs:0\", shape=(?, 117), dtype=float32)\n",
      "Tensor(\"labels:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"learning_rate:0\", shape=(), dtype=float32)\n",
      "<tf.Variable 'global_step:0' shape=() dtype=int32_ref>\n"
     ]
    }
   ],
   "source": [
    "# Data inputs\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, training_data.shape[1]], name=\"inputs\")\n",
    "print(x)\n",
    "# Labels inputs\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"labels\")\n",
    "print(y)\n",
    "\n",
    "# Learning rate input\n",
    "learning_rate_input = tf.placeholder(dtype=tf.float32, shape=[], name=\"learning_rate\")\n",
    "print(learning_rate_input)\n",
    "\n",
    "# Used in training to keep track of variables updates\n",
    "global_step = tf.get_variable(name=\"global_step\",\n",
    "                              shape=[],\n",
    "                              dtype=tf.int32,\n",
    "                              initializer=tf.constant_initializer(0),\n",
    "                              trainable=False)\n",
    "print(global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = fully_connected_layers\n",
    "\n",
    "# Create first dense layer\n",
    "# Shape: (batch_size, fully_connected_size)\n",
    "dense = tf.layers.dense(x,\n",
    "                        units=fully_connected_size,\n",
    "                        activation=fully_connected_activation,\n",
    "                        name=\"dense_1\")\n",
    "\n",
    "dense_layers -= 1\n",
    "dense_layer_count = 2\n",
    "\n",
    "# Create next dense layers\n",
    "for i in range(dense_layers):\n",
    "    # Shape: (batch_size, fully_connected_size)\n",
    "    dense = tf.layers.dense(dense,\n",
    "                            units=fully_connected_size,\n",
    "                            activation=fully_connected_activation,\n",
    "                            name=\"dense_{}\".format(dense_layer_count))\n",
    "\n",
    "    dense_layer_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape: (batch_size, 1)\n",
    "output_layer = tf.layers.dense(dense, \n",
    "                               units=1, \n",
    "                               activation=tf.nn.sigmoid,\n",
    "                               name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and Optimizer\n",
    "\n",
    "Now we create the tensorflow operations to calculate the cost of the model and perform one training step. We will use gradient descent to optimize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"cost_and_optimizer\"):\n",
    "    \n",
    "    # Loss value for the model output of each data point fed to the graph\n",
    "    # Shape: (batch_size, 1)\n",
    "    loss_op = tf.nn.sigmoid_cross_entropy_with_logits(labels=y,\n",
    "                                                      logits=output_layer,\n",
    "                                                      name=\"loss_op\")\n",
    "    \n",
    "    # Cost value for all the data points fed to the model\n",
    "    # Shape: ()\n",
    "    cost = tf.reduce_sum(loss_op, name=\"cost\")\n",
    "    \n",
    "    # Model optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_input,\n",
    "                                                 name=\"gradient_descent_optimizer\")\n",
    "    \n",
    "    # Trining operation. Perform one parameters update. Increment global_step by 1.\n",
    "    training_op = optimizer.minimize(cost, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model ouputs a value between 0 and 1. We need to transform \n",
    "# it to the predicted label (0 or 1)\n",
    "predictions = tf.round(output_layer, name=\"predictions\")\n",
    "\n",
    "# Metrics\n",
    "with tf.name_scope(\"metrics\"):\n",
    "    # Get the model accuracy.\n",
    "    accuracy = tf.metrics.accuracy(labels=y,\n",
    "                                   predictions=predictions,\n",
    "                                   name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_logs_dir = Path(Path.cwd(), \"tensorboard_logs\")\n",
    "tensorboard_logs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy[0])\n",
    "    tf.summary.histogram(\"output_layer\", output_layer)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model training\n",
    "\n",
    "Now it's time to train the model!\n",
    "\n",
    "We will feed the data to the graph, one batch at a time. We will perform a parameters update each time we feed a batch ([mini batch gradient descent](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)).\n",
    "\n",
    "Every some training steps, we will evaluate the model on the validation data, to check how the model is performing on data not used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lists of batchs\n",
    "\n",
    "The batches are generated using the DataIterator of this repository. It outputs a batch of the required size at a time. It can shuffle the data before creating the batches, so the composition of each batch is different for each epoch (full pass over the entire training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-928c3c08b008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m training_iterator = DataIterator(data=training_data,\n\u001b[1;32m      2\u001b[0m                                  \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                                  shuffle=True)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "training_iterator = DataIterator(data=training_data,\n",
    "                                 labels=training_labels,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True)\n",
    "\n",
    "validation_iterator = DataIterator(data=validation_data,\n",
    "                                   labels=validation_labels,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hyperparameters\n",
    "\n",
    "These are the hyperparameters we need to tune in order to get the model and training to work. Try different values to see how the training and model performance change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training operations\n",
    "\n",
    "Now that we have the model graph, the training operations, and the data batch generators, we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful variables\n",
    "# Validate the model every some global steps and print information\n",
    "val_period = 10\n",
    "# Save the model to file every some global steps\n",
    "save_period = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard files\n",
    "tensorboard_job_name = \"dl_{}-ds_{}-lr_{}-e_{}-b_{}\".format(fully_connected_layers,\n",
    "                                                            fully_connected_size,\n",
    "                                                            learning_rate,\n",
    "                                                            epochs,\n",
    "                                                            batch_size)\n",
    "tensorboard_log_dir = Path(tensorboard_logs_dir, tensorboard_job_name)\n",
    "\n",
    "# This tensorflow operations are used to initialize all the variables of the model.\n",
    "init_op = tf.global_variables_initializer()\n",
    "init_l_op = tf.local_variables_initializer()\n",
    "\n",
    "global_step_count = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize graph variables\n",
    "    sess.run(init_op)\n",
    "    sess.run(init_l_op)\n",
    "    \n",
    "    # Training and validation Tensorboard files writers\n",
    "    train_writer = tf.summary.FileWriter(str(Path(tensorboard_log_dir, \"train\")),\n",
    "                                         sess.graph)\n",
    "    validation_writer = tf.summary.FileWriter(str(Path(tensorboard_log_dir, \"validation\")),\n",
    "                                              sess.graph)\n",
    "    \n",
    "    # Set up a Saver for periodically serializing the model\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "    \n",
    "    # Create list to store usefull data\n",
    "    training_costs = []\n",
    "    training_accuracies = []\n",
    "    validation_costs = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    # Iterate over the entire data\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Iterate over a generator that returns batches\n",
    "        for train_batch in training_iterator:\n",
    "            \n",
    "            global_step_count = sess.run(global_step)\n",
    "            \n",
    "            feed_dict = {x: train_batch[0],\n",
    "                         y: train_batch[1],\n",
    "                         learning_rate_input: learning_rate}\n",
    "            \n",
    "            # Do a gradient update, and log results to Tensorboard               \n",
    "            train_cost, train_accuracy, _, train_summary = sess.run(\n",
    "                [cost, accuracy, training_op, summary_op],\n",
    "                feed_dict=feed_dict)\n",
    "            train_writer.add_summary(train_summary, global_step_count)\n",
    "            \n",
    "            training_costs.append(train_cost)\n",
    "            training_accuracies.append(train_accuracy[0])\n",
    "            \n",
    "            # Evaluate the model on validation set every val_period steps\n",
    "            if global_step_count % val_period == 0:\n",
    "                \n",
    "                val_batch_accuracies = []\n",
    "                val_batch_costs = []\n",
    "                \n",
    "                # Evaluate on validation data\n",
    "                for val_batch in validation_iterator:\n",
    "                    \n",
    "                    validation_feed_dict = {x: val_batch[0],\n",
    "                                            y: val_batch[1]}\n",
    "                    \n",
    "                    val_batch_accuracy, val_batch_cost = sess.run(\n",
    "                        [accuracy, cost],\n",
    "                        feed_dict=validation_feed_dict)\n",
    "                    \n",
    "                    val_batch_accuracies.append(np.sum(val_batch_accuracy[0]))\n",
    "                    val_batch_costs.append(np.sum(val_batch_cost))\n",
    "                \n",
    "                mean_val_accuracy = np.sum(np.array(val_batch_accuracies)) / validation_iterator.total_batches\n",
    "                mean_val_cost = np.sum(np.array(val_batch_costs)) / validation_iterator.total_batches\n",
    "                \n",
    "                val_summary = tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"val_summaries/accuracy\",\n",
    "                                     simple_value=mean_val_accuracy),\n",
    "                    tf.Summary.Value(tag=\"val_summaries/cost\",\n",
    "                                     simple_value=mean_val_cost)])\n",
    "                \n",
    "                validation_writer.add_summary(val_summary, global_step_count)\n",
    "                \n",
    "                validation_costs.append(mean_val_cost)\n",
    "                validation_accuracies.append(mean_val_accuracy)\n",
    "                \n",
    "                # Print information abuot training\n",
    "                print(\"Epoch {} - Global step {} - Training cost {} - Training accuracy {} - \"\n",
    "                      \"Validation cost {} - Validation accuracy {}\".format(epoch,\n",
    "                                                                           global_step_count,\n",
    "                                                                           train_cost,\n",
    "                                                                           train_accuracy[0],\n",
    "                                                                           mean_val_cost,\n",
    "                                                                           mean_val_accuracy))\n",
    "            \n",
    "            # Write a model checkpoint if necessary.\n",
    "            if global_step_count % save_period == 0:\n",
    "                save_path = saver.save(sess, \"trained_models/model.ckpt\")\n",
    "                print(\"Model saved in path: {}\".format(save_path))\n",
    "    \n",
    "    # Done training!\n",
    "    print(\"Finished {} epochs!\".format(epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot cost and accuracy during training\n",
    "\n",
    "Now we plot the how the cost and accuracy change during training, for both the training data and the validation data. This information is also ploted in Tensorboard, but we plot it here anyways.\n",
    "\n",
    "*Note: The training cost and accuracy is for one batch of data, but for the validation test, we plot the mean accuracy and cost over all the data batches.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(training_costs)), training_costs, color=\"red\")\n",
    "plt.title(\"Training cost\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(training_accuracies)), training_accuracies, color=\"red\")\n",
    "plt.title(\"Training accuracy\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(validation_costs)), validation_costs)\n",
    "plt.title(\"Validation cost\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(validation_accuracies)), validation_accuracies)\n",
    "plt.title(\"Validation accuracy\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "To use tensorboard open a console, go to the project folder (`\"mushrooms\"`) type:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir tensoboard_logs\\\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensoboard results\n",
    "\n",
    "![output_layer_histograms](notebook_images/output_layer_histograms.png)\n",
    "![graph](notebook_images/graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
